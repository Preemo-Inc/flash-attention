[tool.poetry]
name = "flash_attn_v1"
version = "1.0.8"
description = "Flash Attention: Fast and Memory-Efficient Exact Attention"
authors = ["Tri Dao <trid@stanford.edu>"]

[tool.poetry.build]
script = "build.py"
generate-setup-file = true

[build-system]
requires = [
  "poetry-core",
  "setuptools",
  "einops",
  "packaging",
  "ninja",
  "wheel",
  "torch==2.0.1+cu118",
]
build-backend = "poetry.core.masonry.api"
